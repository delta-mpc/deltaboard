{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Example of the Horizontal Federated Learning Task\n",
    "\n",
    "This is an example of running horizontal federated learning Delta Task on multiple Delta Nodes.\n",
    "\n",
    "The data ([MNIST Dataset](http://yann.lecun.com/exdb/mnist/)) is distributed on several nodes with each node only having partial dataset.\n",
    "And the task is to train a Convolutional Neural Network model to identify hand-writing digits.\n",
    "\n",
    "This example could be executed in Deltaboard directly. <span style=\"color:#FF8F8F;font-weight:bold\">Before hitting the run button, the Delta Node API address should be modified according to the user's config, the instructions are explained in section 4 below.</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the Required Packages\n",
    "\n",
    "The computation logic is written in Torch. So we must import ```numpy``` and ```torch```, and some other helper tools. Then we need to import Delta Task framework components from Python package ```delta-task``` including ```DeltaNode``` for Delta Node API connection, the ```HorizontalLearning``` for defination of the horizontal learning task and the ```FaultTolerantFedAvg``` for the configuration of the secure aggregation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Iterable, List, Tuple\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL.Image import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from delta.delta_node import DeltaNode\n",
    "from delta.task.learning import HorizontalLearning, FaultTolerantFedAvg\n",
    "import delta.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the Neural Network Model\n",
    "\n",
    "Now let's define the CNN model, which is exactly the same as what we will do before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LeNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 16, 5, padding=2)\n",
    "        self.pool1 = torch.nn.AvgPool2d(2, stride=2)\n",
    "        self.conv2 = torch.nn.Conv2d(16, 16, 5)\n",
    "        self.pool2 = torch.nn.AvgPool2d(2, stride=2)\n",
    "        self.dense1 = torch.nn.Linear(400, 100)\n",
    "        self.dense2 = torch.nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = x.view(-1, 400)\n",
    "        x = self.dense1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the Horizontal Federated Learning Task\n",
    "\n",
    "The next step is to define our horizontal federated learning task to train the above model on multiple nodes.\n",
    "\n",
    "There're several parts in the PPC Task that need to be programmed by the developer:\n",
    "\n",
    "* ***Task Config***: We can make some basis task config in the ```super().__init__()``` method. The configurations involves task name (```name```), training rounds of task (```max_rounds```), the frequency of validation (validate per ```validate_interval``` round), validate dataset fraction (```validate_frac```) and the aggregate strategy (```strategy```). \n",
    "* ***Dataset***: In the ```dataset``` method, you can specify the dataset for task. You should return an instance of ```delta.dataset.Dataset```, and the parameter ```dataset``` of ```delta.dataset.Dataset``` represents the dataset name. For detailed explanation of the dataset format, please refer to [this document](https://docs.deltampc.com/network-deployment/prepare-data).\n",
    "* ***Train dataloader method***: In the ```make_train_dataloader``` method, you can specify the dataloader used for training. We will pass the training dataset (an instance of ```torch.utils.data.Dataset```) to this method according to the configuration in the ```dataset``` method, and you can transform the dataset, do some preprocess, etc. And finally you should return a ```torch.utils.data.Dataloader```, and it will be used for model training.\n",
    "* ***Validation dataloader method***: In the ```make_validate_dataloader``` method, you can specify the dataloader used for validation. The implementation of this method is very similar with the ```make_train_dataloader```, except of the passed dataset is the validation dataset.\n",
    "* ***Model Training Method***: In the ```train```method, you should define the whole procedure of model training, including forward propagation and backward propagation. The input parameter of this method is the traing dataloader.\n",
    "* ***Model Validation Method***: In the ```validate``` method, you should define the whole procedure of model validation. The input parameter of this method is the validation dataloader, and the return value should be a ```dict``` of which key should be the validation metrics name, and corresponding value should be the metrics value.\n",
    "* ***State dict***: In the ```state_dict``` method, you can specify all the model parameters need to train and update, and the return value should be a list of these parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(data: List[Tuple[Image, str]]):\n",
    "    \"\"\"\n",
    "    Used as the collate_fn of dataloader to preprocess the data.\n",
    "    Resize, normalize the input mnist image, and the return it as a torch.Tensor.\n",
    "    \"\"\"\n",
    "    xs, ys = [], []\n",
    "    for x, y in data:\n",
    "        xs.append(np.array(x).reshape((1, 28, 28)))\n",
    "        ys.append(int(y))\n",
    "\n",
    "    imgs = torch.tensor(xs)\n",
    "    label = torch.tensor(ys)\n",
    "    imgs = imgs / 255 - 0.5\n",
    "    return imgs, label\n",
    "\n",
    "\n",
    "class Example(HorizontalLearning):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(\n",
    "            name=\"example\",  # The task name which is used for displaying purpose.\n",
    "            max_rounds=2,  # The number of total rounds of training. In every round, all the nodes calculate their own partial results, and summit them to the server.\n",
    "            validate_interval=1,  # The number of rounds after which we calculate a validation score.\n",
    "            validate_frac=0.1,  # The ratio of samples for validate set in the whole dataset，range in (0,1)\n",
    "            strategy=FaultTolerantFedAvg(  # Strategy for secure aggregation, now available strategies are FedAvg and FaultTolerantFedAvg, in package delta.task.learning\n",
    "                min_clients=2,  # Minimum nodes required in each round, must be greater than 2.\n",
    "                max_clients=3,  # Maximum nodes allowed in each round, must be greater equal than min_clients.\n",
    "                merge_epoch=1,  # The number of epochs to run before aggregation is performed.\n",
    "                merge_iteration=0, # The number of iterations to run before aggregation is performed. One of this and the above number must be 0.\n",
    "                wait_timeout=30,  # Timeout for calculation.\n",
    "                connection_timeout=10  # Wait timeout for each step.\n",
    "            )\n",
    "        )\n",
    "        self.model = LeNet()\n",
    "        self.loss_func = torch.nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.SGD(\n",
    "            self.model.parameters(),\n",
    "            lr=0.1,\n",
    "            momentum=0.9,\n",
    "            weight_decay=1e-3,\n",
    "            nesterov=True,\n",
    "        )\n",
    "\n",
    "    def dataset(self) -> delta.dataset.Dataset:\n",
    "        \"\"\"\n",
    "        Define the dataset for task.\n",
    "        return: an instance of delta.dataset.Dataset\n",
    "        \"\"\"\n",
    "        return delta.dataset.Dataset(dataset=\"mnist\")\n",
    "\n",
    "    def make_train_dataloader(self, dataset: Dataset) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Define the training dataloader. You can transform the dataset, do some preprocess to the dataset.\n",
    "        dataset: training dataset\n",
    "        return: training dataloader\n",
    "        \"\"\"\n",
    "        return DataLoader(dataset, batch_size=64, shuffle=True, drop_last=True, collate_fn=transform_data)  # type: ignore\n",
    "\n",
    "    def make_validate_dataloader(self, dataset: Dataset) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Define the validation dataloader. You can transform the dataset, do some preprocess to the dataset.\n",
    "        dataset: validation dataset\n",
    "        return: validation dataloader\n",
    "        \"\"\"\n",
    "        return DataLoader(dataset, batch_size=64, shuffle=False, drop_last=False, collate_fn=transform_data)  # type: ignore\n",
    "\n",
    "    def train(self, dataloader: Iterable):\n",
    "        \"\"\"\n",
    "        The training step defination.\n",
    "        dataloader: the dataloader corresponding to the dataset.\n",
    "        return: None\n",
    "        \"\"\"\n",
    "        for batch in dataloader:\n",
    "            x, y = batch\n",
    "            y_pred = self.model(x)\n",
    "            loss = self.loss_func(y_pred, y)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def validate(self, dataloader: Iterable) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Validation method.\n",
    "        To calculate validation scores on each node after several training steps.\n",
    "        The result will also go through the secure aggregation before sending back to server.\n",
    "        dataloader: the dataloader corresponding to the dataset.\n",
    "        return: Dict[str, float], A dictionary with each key (str) corresponds to a score's name and the value (float) to the score's value.\n",
    "        \"\"\"\n",
    "        total_loss = 0\n",
    "        count = 0\n",
    "        ys = []\n",
    "        y_s = []\n",
    "        for batch in dataloader:\n",
    "            x, y = batch\n",
    "            y_pred = self.model(x)\n",
    "            loss = self.loss_func(y_pred, y)\n",
    "            total_loss += loss.item()\n",
    "            count += 1\n",
    "\n",
    "            y_ = torch.argmax(y_pred, dim=1)\n",
    "            y_s.extend(y_.tolist())\n",
    "            ys.extend(y.tolist())\n",
    "        avg_loss = total_loss / count\n",
    "        tp = len([1 for i in range(len(ys)) if ys[i] == y_s[i]])\n",
    "        precision = tp / len(ys)\n",
    "\n",
    "        return {\"loss\": avg_loss, \"precision\": precision}\n",
    "\n",
    "    def state_dict(self) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        The params that need to train and update.\n",
    "        Only the params returned by this function will be updated and saved during aggregation.\n",
    "        return: List[torch.Tensor]， The list of model params.\n",
    "        \"\"\"\n",
    "        return self.model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set the API Address of the Delta Node\n",
    "\n",
    "After defining the task details, we're ready to run the task on the Delta Nodes.\n",
    "\n",
    "Delta Task framework could send the task to Delta Node directly, as long as the Delta Node API address is specified.\n",
    "\n",
    "Here we use the Delta Node API provided by Deltaboard. Deltaboard provides a separate API address for each of its users, the tasks submitted via the API could be listed inside Deltaboard. The developer could also use API from Delta Node directly.\n",
    "\n",
    "Click \"Profiles\" on the sidebar of Deltaboard, copy the API Address in Deltaboard API section, and paste it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELTA_NODE_API = \"http://127.0.0.1:6704\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run the PPC Task\n",
    "\n",
    "Finally we can start the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = Example().build()\n",
    "delta_node = DeltaNode(DELTA_NODE_API)\n",
    "delta_node.create_task(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Check the Running Status\n",
    "\n",
    "After clicking the run button, some logs will be print out showing the task is submitted to the Delta Node successfully.\n",
    "\n",
    "To see the task execution details, go to \"My Tasks\" on the sidebar of Deltaboard, the task should be listed.\n",
    "Click the item to view the execution logs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
